{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TxMM Assignment 4 - Authorship Attribution\n",
    "\n",
    "This "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import statistics\n",
    "import math\n",
    "\n",
    "from typing import List\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import nltk.tokenize.destructive\n",
    "import contractions\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"tagsets\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "\n",
    "data_dir = os.path.join(os.getcwd(), \"Data\")\n",
    "\n",
    "def load_fanfiction_data(data_purpose: str) -> pd.DataFrame:\n",
    "    path_to_data_csv = os.path.join(data_dir, f\"pan2324_{data_purpose}_data.csv\")\n",
    "    \n",
    "    return pd.read_csv(path_to_data_csv, index_col=0).sort_index()\n",
    "\n",
    "train_data = load_fanfiction_data(\"train\")\n",
    "test_data = load_fanfiction_data(\"test\")\n",
    "dev_data = load_fanfiction_data(\"dev\")\n",
    "\n",
    "train_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text preprocessing\n",
    "\n",
    "In order to extract features from the text data, it might be required that some preprocessing steps are taken first. To this end, a collection of text preprocessing functions are defined here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_punctuation(word_tokenized_text: list[str]) -> list[str]:\n",
    "    word_tokenized_text_no_punctuation = []\n",
    "    \n",
    "    for word in word_tokenized_text:\n",
    "        if word in list(string.punctuation):\n",
    "            continue\n",
    "        \n",
    "        word_tokenized_text_no_punctuation.append(word)\n",
    "        \n",
    "    return word_tokenized_text_no_punctuation\n",
    "\n",
    "def _remove_digits(word_tokenized_text: list[str]) -> list[str]:\n",
    "    word_tokenized_text_no_digits = []\n",
    "    \n",
    "    for word in word_tokenized_text:\n",
    "        if word in list(string.digits):\n",
    "            continue\n",
    "        \n",
    "        word_tokenized_text_no_digits.append(word)\n",
    "        \n",
    "    return word_tokenized_text_no_digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature extraction\n",
    "\n",
    "For this assignment, a set of different features are used. These features may be split up into a number of feature categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_text = \"I'm looking for a research project where I can study the language Esperanto from a linguistic perspective. That subject's called Esperantology. I think that's quite nifty :). 10/10 idea. Gabbagool!!!\"\n",
    "val_text_word_tokenized = nltk.word_tokenize(val_text)\n",
    "val_text_sent_tokenized = nltk.sent_tokenize(val_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Count features\n",
    "\n",
    "Count features are lexical features that the describe the count of something within the text, like the count of words or sentences in a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def character_count(text: str) -> int:\n",
    "    \"\"\"Returns the count of all characters of the given parameter string.\n",
    "    \n",
    "    Args:\n",
    "        - `text`: The string to compute the amount of characters of.\n",
    "        \n",
    "    Returns:\n",
    "        The count of all characters in the `text` parameter.\n",
    "    \"\"\"\n",
    "    \n",
    "    return len(text)\n",
    "\n",
    "def word_count(word_tokenized_text: str) -> int:\n",
    "    \"\"\"Returns the count of all words that occur in the input string. \n",
    "    This count is determined using NLTK's `word_tokenize` function.\n",
    "    \n",
    "    Args:\n",
    "        - `word_tokenized_text`: The string to compute the amount of words of.\n",
    "        \n",
    "    Returns:\n",
    "        The count of all words in the `word_tokenized_text` parameter.\n",
    "    \"\"\"\n",
    "    \n",
    "    return len(word_tokenized_text)\n",
    "\n",
    "def sentence_count(sentence_tokenized_text: str) -> int:\n",
    "    \"\"\"Returns the count of all sentences that occur in the input string. \n",
    "    This count is determined using NLTK's `sent_tokenize` function.\n",
    "    \n",
    "    Args:\n",
    "        - `sentence_tokenized_text`: The string to compute the amount of sentences of.\n",
    "        \n",
    "    Returns:\n",
    "        The count of all sentences in the `sentence_tokenized_text` parameter.\n",
    "    \"\"\"\n",
    "    return len(sentence_tokenized_text)\n",
    "\n",
    "def punctuation_count(text: str) -> int:\n",
    "    \"\"\"Returns the count of punctuation occurrences in the input string.\n",
    "    \n",
    "    Args:\n",
    "        - `text`: The string to compute the amount of punctuation occurrences of.\n",
    "        \n",
    "    Returns:\n",
    "        The number of times a form of punctuation occurs in the `text` parameter.    \n",
    "    \"\"\"\n",
    "    \n",
    "    punctuation_count = 0\n",
    "    \n",
    "    for character in text:\n",
    "        if character in list(string.punctuation):\n",
    "            punctuation_count += 1\n",
    "    \n",
    "    return punctuation_count\n",
    "\n",
    "def digit_count(text: str) -> int:\n",
    "    \"\"\"Returns the count of individual digits occurring in the input string.\n",
    "    Note that this does not mean numbers, e.g., \"23\" will return \"2\", since the number 23 consists of two digits.\n",
    "    \n",
    "    Args:\n",
    "        - `text`: The string to compute the amount of digit occurrences of.\n",
    "        \n",
    "    Returns:\n",
    "        The number of times a digit occurs in the `text` parameter.\n",
    "    \"\"\"\n",
    "    digit_count = 0\n",
    "    \n",
    "    for character in text:\n",
    "        if character in list(string.digits):\n",
    "            digit_count += 1\n",
    "            \n",
    "    return digit_count\n",
    "\n",
    "def uppercase_count(text: str) -> int:\n",
    "    \"\"\"Returns the count of uppercase characters occurring in the input string.\n",
    "    Note that only uppercase characters that are part of ASCII are supported.\n",
    "    \n",
    "    Args:\n",
    "        - `text`: The string to compute the amount of uppercase character occurrences of.\n",
    "        \n",
    "    Returns:\n",
    "        The number of times an uppercase character occurs in the `text` parameter.\n",
    "    \"\"\"\n",
    "    uppercase_count = 0\n",
    "    \n",
    "    for character in text:\n",
    "        if character in list(string.ascii_uppercase):\n",
    "            uppercase_count += 1\n",
    "            \n",
    "    return uppercase_count\n",
    "\n",
    "def short_word_count(word_tokenized_text: List[str], short_word_max_length: int = 4) -> int:\n",
    "    \"\"\"Returns the count of \"short\" words that occur in the `word_tokenized_text` parameter.\n",
    "    The cutoff point for \"short\" words is given by the `short_word_max_length` parameter.\n",
    "    \n",
    "    Args:\n",
    "        - `word_tokenized_text`: The string to compute the amount of \"short\" words of.\n",
    "        - `short_word_max_length`: The maximum length of what is considered to be a \"short\" word. This length is inclusive.\n",
    "        \n",
    "    Returns:\n",
    "        The number of times a \"short\" word occurs in the `word_tokenized_text` parameter.\n",
    "    \"\"\"     \n",
    "    short_word_list = [word for word in word_tokenized_text if len(word) <= short_word_max_length]\n",
    "    \n",
    "    return len(short_word_list)\n",
    "\n",
    "def alphabet_count(text: List[str], include_uppercase: bool = True, include_punctuation: bool = True, include_digits: bool = True) -> int:\n",
    "    \"\"\"Returns the length of the alphabet of the given text. A text's alphabet is defined as all unique characters that occur in that text.\n",
    "    \n",
    "    Args:\n",
    "        - `text`: The string for which to compute an alphabet for.\n",
    "        - `include_uppercase`: A boolean used to determine whether to count uppercase characters as separate from their lowercase counterparts.\n",
    "        - `include_punctuation`: A boolean used to determine whether to include punctuation in the alphabet.\n",
    "        - `include_digits`: A boolean used to determine whether to include digits in the alphabet.\n",
    "        \n",
    "    Returns:\n",
    "        The length of the alphabet of the `text` variable.\n",
    "    \"\"\"\n",
    "    if not include_uppercase:\n",
    "        text = text.lower()\n",
    "    \n",
    "    text_char_alphabet = set(text)\n",
    "    \n",
    "    if not include_punctuation:\n",
    "        text_char_alphabet = {char for char in text_char_alphabet if char not in list(string.punctuation)}\n",
    "        \n",
    "    if not include_digits:\n",
    "        text_char_alphabet = {char for char in text_char_alphabet if char not in list(string.digits)}\n",
    "    \n",
    "    return len(text_char_alphabet)\n",
    "\n",
    "def contraction_count(text: List[str], include_genetive_count: bool = True) -> int:\n",
    "    \"\"\"Returns the count of all contractions that occur in the given string.\n",
    "    \n",
    "    Args:\n",
    "        - `text`: The string for which to count the number of occurring contractions.\n",
    "        - `include_genetive_count`: A boolean used to determine if occurrences of the genetive should count towards the number of contractions found.\n",
    "        \n",
    "    Returns:\n",
    "        The amount of contractions that occur in the `text` variable.\n",
    "    \"\"\"\n",
    "    contraction_count = len(contractions.preview(text, 1))\n",
    "    \n",
    "    if include_genetive_count:\n",
    "        tokenized_text = nltk.tokenize.word_tokenize(text)\n",
    "        pos_tagged_text = nltk.tag.pos_tag(tokenized_text)\n",
    "        genetive_count = len([tag for _, tag in pos_tagged_text if tag == \"POS\"])\n",
    "        \n",
    "        contraction_count += genetive_count\n",
    "        \n",
    "    return contraction_count\n",
    "\n",
    "def word_without_vowels_count(word_tokenized_text: List[str], include_y_as_vowel: bool = False) -> int:\n",
    "    \"\"\"Returns the count of words in the input string that do not contain vowels.\n",
    "    \n",
    "    Args:\n",
    "        - `word_tokenized_text`: The string for which to count the number of words without vowels.\n",
    "        - `include_y_as_vowel`: A boolean used to determine if the character \"y\" should be counted as a vowel.\n",
    "        \n",
    "    Returns:\n",
    "        The number of times a word without vowels occurs in the input string.\n",
    "    \"\"\"\n",
    "    \n",
    "    word_without_vowels_count = 0\n",
    "    vowels = set(\"aeiou\")\n",
    "    \n",
    "    if include_y_as_vowel:\n",
    "        vowels.add(\"y\")\n",
    "    \n",
    "    word_tokenized_text = _remove_punctuation(word_tokenized_text)\n",
    "    word_tokenized_text = _remove_digits(word_tokenized_text)\n",
    "    \n",
    "    for word in word_tokenized_text:        \n",
    "        if len(vowels.intersection(word)) == 0:\n",
    "            word_without_vowels_count += 1\n",
    "    \n",
    "    return word_without_vowels_count\n",
    "\n",
    "def hapax_legomenon_count(word_tokenized_text: List[str]) -> int:\n",
    "    \"\"\"Returns the count of hapax legomenon in the input text.\n",
    "    A hapax legomenon is a word that occurs only once in a corpus.\n",
    "    Note that for the purposes of this function, the corpus is considered to be the input text.\n",
    "    \n",
    "    Args:\n",
    "        - `word_tokenized_text`: The string for which to count the number of hapax legomenon.\n",
    "        \n",
    "    Returns:\n",
    "        The number of hapax legomena that were found in the input text.\n",
    "    \"\"\"\n",
    "    \n",
    "    hapax_legomenon_count = 0\n",
    "    \n",
    "    word_tokenized_text = [word.lower() for word in word_tokenized_text]\n",
    "        \n",
    "    for word in word_tokenized_text:      \n",
    "        if word_tokenized_text.count(word) == 1:\n",
    "            hapax_legomenon_count += 1\n",
    "            \n",
    "    return hapax_legomenon_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Text complexity features\n",
    "\n",
    "Text complexity features aim to describe the complexity of the given text. This may be at the lexical or the syntactical level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_word_length(word_tokenized_text: List[str]) -> int:   \n",
    "    word_lengths = [len(word) for word in word_tokenized_text]\n",
    "    \n",
    "    return round(statistics.mean(word_lengths), 3)\n",
    "\n",
    "def mean_sentence_length(word_tokenized_text: List[str]) -> int:   \n",
    "    sentence_lengths = [len(sentence) for sentence in word_tokenized_text]\n",
    "    \n",
    "    return round(statistics.mean(sentence_lengths), 3)\n",
    "\n",
    "def word_length_standard_deviation(word_tokenized_text: List[str]) -> int:    \n",
    "    word_lengths = [len(word) for word in word_tokenized_text]\n",
    "    \n",
    "    return round(statistics.stdev(word_lengths), 3)\n",
    "\n",
    "def sentence_length_standard_deviation(word_tokenized_text: List[str]) -> int:    \n",
    "    sentence_lengths = [len(sentence) for sentence in word_tokenized_text]\n",
    "    \n",
    "    return round(statistics.stdev(sentence_lengths), 3)\n",
    "\n",
    "def mean_word_frequency(word_tokenized_text: List[str]) -> int:    \n",
    "    word_frequencies = {}\n",
    "    \n",
    "    for word in word_tokenized_text:\n",
    "        if word not in word_frequencies.keys():\n",
    "            word_frequencies[word] = 1\n",
    "        else:\n",
    "            word_frequencies[word] += 1\n",
    "                        \n",
    "    return round(statistics.mean(word_frequencies.values()), 3)\n",
    "\n",
    "def lexical_diversity_coefficient(word_tokenized_text: List[str]) -> int:\n",
    "    \"\"\"From http://repository.utm.md/handle/5014/20225\"\"\"\n",
    "    total_word_count = word_count(word_tokenized_text)\n",
    "    unique_word_count = hapax_legomenon_count(word_tokenized_text)\n",
    "    \n",
    "    lexical_diversity_coefficient = unique_word_count / total_word_count\n",
    "    \n",
    "    return round(lexical_diversity_coefficient, 3)\n",
    "\n",
    "def syntactic_complexity_coefficient(word_tokenized_text: List[str]) -> int:\n",
    "    \"\"\"From http://repository.utm.md/handle/5014/20225\"\"\"\n",
    "    total_word_count = word_count(word_tokenized_text)\n",
    "    total_sentence_count = sentence_count(word_tokenized_text)\n",
    "    \n",
    "    syntactic_complexity_coefficient = 1 - total_sentence_count / total_word_count\n",
    "    \n",
    "    return round(syntactic_complexity_coefficient, 3)\n",
    "\n",
    "def herdans_log_type_token_richness(word_tokenized_text: List[str]) -> int:\n",
    "    \"\"\"From https://pubs.asha.org/doi/abs/10.1044/jshr.3203.536\"\"\"\n",
    "    total_word_count = word_count(word_tokenized_text)\n",
    "    unique_word_count = hapax_legomenon_count(word_tokenized_text)\n",
    "    \n",
    "    herdans_log_type_token_richness = math.log(unique_word_count) / math.log(total_word_count)\n",
    "    \n",
    "    return round(herdans_log_type_token_richness, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Part-of-speech features\n",
    "\n",
    "These features describe the ratios of words that belong to a specific part-of-speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pos_tag_ratio(text_word_tokenized: List[str], pos_tags: List[str]) -> int:\n",
    "    pos_tagged_text = nltk.pos_tag(text_word_tokenized)\n",
    "    total_word_count = len(pos_tagged_text)\n",
    "    pos_tag_count = 0\n",
    "    \n",
    "    for _, tag in pos_tagged_text:\n",
    "        if tag in pos_tags:\n",
    "            pos_tag_count += 1\n",
    "            \n",
    "    return pos_tag_count / total_word_count\n",
    "\n",
    "def noun_common_ratio(text_word_tokenized: List[str]) -> int:\n",
    "    return _pos_tag_ratio(text_word_tokenized, (\"NN\", \"NNS\"))\n",
    "\n",
    "def noun_proper_ratio(text_word_tokenized: List[str]) -> int:\n",
    "    return _pos_tag_ratio(text_word_tokenized, (\"NNP\", \"NNPS\"))\n",
    "\n",
    "def adjective_base_ratio(text_word_tokenized: List[str]) -> int:\n",
    "    return _pos_tag_ratio(text_word_tokenized, (\"JJ\"))\n",
    "\n",
    "def adjective_comparative_ratio(text_word_tokenized: List[str]) -> int:\n",
    "    return _pos_tag_ratio(text_word_tokenized, (\"JJR\"))\n",
    "\n",
    "def adjective_superlative_ratio(text_word_tokenized: List[str]) -> int:\n",
    "    return _pos_tag_ratio(text_word_tokenized, (\"JJS\"))\n",
    "\n",
    "def adverb_base_ratio(text_word_tokenized: List[str]) -> int:\n",
    "    return _pos_tag_ratio(text_word_tokenized, (\"RB\"))\n",
    "\n",
    "def adverb_comparative_ratio(text_word_tokenized: List[str]) -> int:\n",
    "    return _pos_tag_ratio(text_word_tokenized, (\"RBR\"))\n",
    "\n",
    "def adverb_superlative_ratio(text_word_tokenized: List[str]) -> int:\n",
    "    return _pos_tag_ratio(text_word_tokenized, (\"RBS\"))\n",
    "\n",
    "def verb_infinitive_ratio(text_word_tokenized: List[str]) -> int:\n",
    "    return _pos_tag_ratio(text_word_tokenized, (\"VB\"))\n",
    "\n",
    "def verb_present_tense_1st_2nd_person_ratio(text_word_tokenized: List[str]) -> int:\n",
    "    return _pos_tag_ratio(text_word_tokenized, (\"VBP\"))\n",
    "\n",
    "def verb_present_tense_3rd_person_ratio(text_word_tokenized: List[str]) -> int:\n",
    "    return _pos_tag_ratio(text_word_tokenized, (\"VBZ\"))\n",
    "\n",
    "def verb_past_tense_ratio(text_word_tokenized: List[str]) -> int:\n",
    "    return _pos_tag_ratio(text_word_tokenized, (\"VBD\"))\n",
    "\n",
    "def verb_present_participle(text_word_tokenized: List[str]) -> int:\n",
    "    return _pos_tag_ratio(text_word_tokenized, (\"VBG\"))\n",
    "\n",
    "def verb_past_participle(text_word_tokenized: List[str]) -> int:\n",
    "    return _pos_tag_ratio(text_word_tokenized, (\"VBN\"))\n",
    "\n",
    "def verb_modal_auxiliary(text_word_tokenized: List[str]) -> int:\n",
    "    return _pos_tag_ratio(text_word_tokenized, (\"MD\"))\n",
    "\n",
    "def pronoun_ratio(text_word_tokenized: List[str]) -> int:\n",
    "    return _pos_tag_ratio(text_word_tokenized, (\"PRP\", \"PRP$\"))\n",
    "\n",
    "def genetive_ratio(text_word_tokenized: List[str]) -> int:\n",
    "    return _pos_tag_ratio(text_word_tokenized, (\"POS\"))\n",
    "\n",
    "def interjection_ratio(text_word_tokenized: List[str]) -> int:\n",
    "    return _pos_tag_ratio(text_word_tokenized, (\"UH\"))\n",
    "\n",
    "def foreign_word_ratio(text_word_tokenized: List[str]) -> int:\n",
    "    return _pos_tag_ratio(text_word_tokenized, (\"FW\"))\n",
    "\n",
    "def numeral_ratio(text_word_tokenized: List[str]) -> int:\n",
    "    return _pos_tag_ratio(text_word_tokenized, (\"CD\"))\n",
    "\n",
    "def parenthesis_ratio(text_word_tokenized: List[str]) -> int:\n",
    "    return _pos_tag_ratio(text_word_tokenized, (\"(\", \")\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analytics base table\n",
    "\n",
    "The analytics base table, ABT for short, is the table that contains all the extracted features for all instances of the dataset. The analytics base table is fed to a machine learning algorithm in order to train a model. Here, the ABT for the fanfiction dataset is constructed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Daan\\Documents\\Projecten\\ru-text-and-multimedia-mining-23-24\\Assignments\\Assignment 4\\A4 Authorship Attribution.ipynb Cell 15\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Daan/Documents/Projecten/ru-text-and-multimedia-mining-23-24/Assignments/Assignment%204/A4%20Authorship%20Attribution.ipynb#X26sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m     abt \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat(dataset\u001b[39m.\u001b[39mindex, feature_df, dataset[\u001b[39m\"\u001b[39m\u001b[39mauthor\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Daan/Documents/Projecten/ru-text-and-multimedia-mining-23-24/Assignments/Assignment%204/A4%20Authorship%20Attribution.ipynb#X26sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m abt\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Daan/Documents/Projecten/ru-text-and-multimedia-mining-23-24/Assignments/Assignment%204/A4%20Authorship%20Attribution.ipynb#X26sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m extract_abt(train_data)\n",
      "\u001b[1;32mc:\\Users\\Daan\\Documents\\Projecten\\ru-text-and-multimedia-mining-23-24\\Assignments\\Assignment 4\\A4 Authorship Attribution.ipynb Cell 15\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Daan/Documents/Projecten/ru-text-and-multimedia-mining-23-24/Assignments/Assignment%204/A4%20Authorship%20Attribution.ipynb#X26sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m features\u001b[39m.\u001b[39mappend(dataset[\u001b[39m\"\u001b[39m\u001b[39mtext_word_tokenized\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mapply(adverb_base_ratio))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Daan/Documents/Projecten/ru-text-and-multimedia-mining-23-24/Assignments/Assignment%204/A4%20Authorship%20Attribution.ipynb#X26sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m features\u001b[39m.\u001b[39mappend(dataset[\u001b[39m\"\u001b[39m\u001b[39mtext_word_tokenized\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mapply(adverb_comparative_ratio))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Daan/Documents/Projecten/ru-text-and-multimedia-mining-23-24/Assignments/Assignment%204/A4%20Authorship%20Attribution.ipynb#X26sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m features\u001b[39m.\u001b[39mappend(dataset[\u001b[39m\"\u001b[39;49m\u001b[39mtext_word_tokenized\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(adverb_superlative_ratio))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Daan/Documents/Projecten/ru-text-and-multimedia-mining-23-24/Assignments/Assignment%204/A4%20Authorship%20Attribution.ipynb#X26sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m features\u001b[39m.\u001b[39mappend(dataset[\u001b[39m\"\u001b[39m\u001b[39mtext_word_tokenized\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mapply(verb_infinitive_ratio))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Daan/Documents/Projecten/ru-text-and-multimedia-mining-23-24/Assignments/Assignment%204/A4%20Authorship%20Attribution.ipynb#X26sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m features\u001b[39m.\u001b[39mappend(dataset[\u001b[39m\"\u001b[39m\u001b[39mtext_word_tokenized\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mapply(verb_present_tense_1st_2nd_person_ratio))\n",
      "File \u001b[1;32mc:\\Users\\Daan\\Documents\\Projecten\\ru-text-and-multimedia-mining-23-24\\env\\Lib\\site-packages\\pandas\\core\\series.py:4760\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4625\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[0;32m   4626\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   4627\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4632\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   4633\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[0;32m   4634\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   4635\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4636\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4751\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4752\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4753\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\n\u001b[0;32m   4754\u001b[0m         \u001b[39mself\u001b[39;49m,\n\u001b[0;32m   4755\u001b[0m         func,\n\u001b[0;32m   4756\u001b[0m         convert_dtype\u001b[39m=\u001b[39;49mconvert_dtype,\n\u001b[0;32m   4757\u001b[0m         by_row\u001b[39m=\u001b[39;49mby_row,\n\u001b[0;32m   4758\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   4759\u001b[0m         kwargs\u001b[39m=\u001b[39;49mkwargs,\n\u001b[1;32m-> 4760\u001b[0m     )\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[1;32mc:\\Users\\Daan\\Documents\\Projecten\\ru-text-and-multimedia-mining-23-24\\env\\Lib\\site-packages\\pandas\\core\\apply.py:1207\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1204\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_compat()\n\u001b[0;32m   1206\u001b[0m \u001b[39m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1207\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[1;32mc:\\Users\\Daan\\Documents\\Projecten\\ru-text-and-multimedia-mining-23-24\\env\\Lib\\site-packages\\pandas\\core\\apply.py:1287\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1281\u001b[0m \u001b[39m# row-wise access\u001b[39;00m\n\u001b[0;32m   1282\u001b[0m \u001b[39m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1283\u001b[0m \u001b[39m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1284\u001b[0m \u001b[39m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1285\u001b[0m \u001b[39m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1286\u001b[0m action \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(obj\u001b[39m.\u001b[39mdtype, CategoricalDtype) \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1287\u001b[0m mapped \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39;49m_map_values(\n\u001b[0;32m   1288\u001b[0m     mapper\u001b[39m=\u001b[39;49mcurried, na_action\u001b[39m=\u001b[39;49maction, convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype\n\u001b[0;32m   1289\u001b[0m )\n\u001b[0;32m   1291\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1292\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1293\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1294\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\Daan\\Documents\\Projecten\\ru-text-and-multimedia-mining-23-24\\env\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[39mreturn\u001b[39;00m arr\u001b[39m.\u001b[39mmap(mapper, na_action\u001b[39m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[39mreturn\u001b[39;00m algorithms\u001b[39m.\u001b[39;49mmap_array(arr, mapper, na_action\u001b[39m=\u001b[39;49mna_action, convert\u001b[39m=\u001b[39;49mconvert)\n",
      "File \u001b[1;32mc:\\Users\\Daan\\Documents\\Projecten\\ru-text-and-multimedia-mining-23-24\\env\\Lib\\site-packages\\pandas\\core\\algorithms.py:1814\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1812\u001b[0m values \u001b[39m=\u001b[39m arr\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m   1813\u001b[0m \u001b[39mif\u001b[39;00m na_action \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1814\u001b[0m     \u001b[39mreturn\u001b[39;00m lib\u001b[39m.\u001b[39;49mmap_infer(values, mapper, convert\u001b[39m=\u001b[39;49mconvert)\n\u001b[0;32m   1815\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1816\u001b[0m     \u001b[39mreturn\u001b[39;00m lib\u001b[39m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1817\u001b[0m         values, mapper, mask\u001b[39m=\u001b[39misna(values)\u001b[39m.\u001b[39mview(np\u001b[39m.\u001b[39muint8), convert\u001b[39m=\u001b[39mconvert\n\u001b[0;32m   1818\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2917\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Daan\\Documents\\Projecten\\ru-text-and-multimedia-mining-23-24\\Assignments\\Assignment 4\\A4 Authorship Attribution.ipynb Cell 15\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Daan/Documents/Projecten/ru-text-and-multimedia-mining-23-24/Assignments/Assignment%204/A4%20Authorship%20Attribution.ipynb#X26sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39madverb_superlative_ratio\u001b[39m(text_word_tokenized: List[\u001b[39mstr\u001b[39m]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mint\u001b[39m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Daan/Documents/Projecten/ru-text-and-multimedia-mining-23-24/Assignments/Assignment%204/A4%20Authorship%20Attribution.ipynb#X26sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m _pos_tag_ratio(text_word_tokenized, (\u001b[39m\"\u001b[39;49m\u001b[39mRBS\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n",
      "\u001b[1;32mc:\\Users\\Daan\\Documents\\Projecten\\ru-text-and-multimedia-mining-23-24\\Assignments\\Assignment 4\\A4 Authorship Attribution.ipynb Cell 15\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Daan/Documents/Projecten/ru-text-and-multimedia-mining-23-24/Assignments/Assignment%204/A4%20Authorship%20Attribution.ipynb#X26sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_pos_tag_ratio\u001b[39m(text_word_tokenized: List[\u001b[39mstr\u001b[39m], pos_tags: List[\u001b[39mstr\u001b[39m]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mint\u001b[39m:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Daan/Documents/Projecten/ru-text-and-multimedia-mining-23-24/Assignments/Assignment%204/A4%20Authorship%20Attribution.ipynb#X26sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     pos_tagged_text \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mpos_tag(text_word_tokenized)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Daan/Documents/Projecten/ru-text-and-multimedia-mining-23-24/Assignments/Assignment%204/A4%20Authorship%20Attribution.ipynb#X26sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     total_word_count \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(pos_tagged_text)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Daan/Documents/Projecten/ru-text-and-multimedia-mining-23-24/Assignments/Assignment%204/A4%20Authorship%20Attribution.ipynb#X26sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     pos_tag_count \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Daan\\Documents\\Projecten\\ru-text-and-multimedia-mining-23-24\\env\\Lib\\site-packages\\nltk\\tag\\__init__.py:165\u001b[0m, in \u001b[0;36mpos_tag\u001b[1;34m(tokens, tagset, lang)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpos_tag\u001b[39m(tokens, tagset\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, lang\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39meng\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    141\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    142\u001b[0m \u001b[39m    Use NLTK's currently recommended part of speech tagger to\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[39m    tag the given list of tokens.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[39m    :rtype: list(tuple(str, str))\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 165\u001b[0m     tagger \u001b[39m=\u001b[39m _get_tagger(lang)\n\u001b[0;32m    166\u001b[0m     \u001b[39mreturn\u001b[39;00m _pos_tag(tokens, tagset, tagger, lang)\n",
      "File \u001b[1;32mc:\\Users\\Daan\\Documents\\Projecten\\ru-text-and-multimedia-mining-23-24\\env\\Lib\\site-packages\\nltk\\tag\\__init__.py:107\u001b[0m, in \u001b[0;36m_get_tagger\u001b[1;34m(lang)\u001b[0m\n\u001b[0;32m    105\u001b[0m     tagger\u001b[39m.\u001b[39mload(ap_russian_model_loc)\n\u001b[0;32m    106\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 107\u001b[0m     tagger \u001b[39m=\u001b[39m PerceptronTagger()\n\u001b[0;32m    108\u001b[0m \u001b[39mreturn\u001b[39;00m tagger\n",
      "File \u001b[1;32mc:\\Users\\Daan\\Documents\\Projecten\\ru-text-and-multimedia-mining-23-24\\env\\Lib\\site-packages\\nltk\\tag\\perceptron.py:167\u001b[0m, in \u001b[0;36mPerceptronTagger.__init__\u001b[1;34m(self, load)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n\u001b[0;32m    165\u001b[0m \u001b[39mif\u001b[39;00m load:\n\u001b[0;32m    166\u001b[0m     AP_MODEL_LOC \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfile:\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(\n\u001b[1;32m--> 167\u001b[0m         find(\u001b[39m\"\u001b[39;49m\u001b[39mtaggers/averaged_perceptron_tagger/\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m PICKLE)\n\u001b[0;32m    168\u001b[0m     )\n\u001b[0;32m    169\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mload(AP_MODEL_LOC)\n",
      "File \u001b[1;32mc:\\Users\\Daan\\Documents\\Projecten\\ru-text-and-multimedia-mining-23-24\\env\\Lib\\site-packages\\nltk\\data.py:530\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    527\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m    529\u001b[0m \u001b[39m# Is the path item a directory or is resource_name an absolute path?\u001b[39;00m\n\u001b[1;32m--> 530\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m path_ \u001b[39mor\u001b[39;00m os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49misdir(path_):\n\u001b[0;32m    531\u001b[0m     \u001b[39mif\u001b[39;00m zipfile \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    532\u001b[0m         p \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(path_, url2pathname(resource_name))\n",
      "File \u001b[1;32m<frozen genericpath>:39\u001b[0m, in \u001b[0;36misdir\u001b[1;34m(s)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def extract_abt(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    features = []\n",
    "    \n",
    "    dataset[\"text_word_tokenized\"] = dataset[\"text\"].apply(nltk.word_tokenize)\n",
    "    dataset[\"text_sent_tokenized\"] = dataset[\"text\"].apply(nltk.sent_tokenize)\n",
    "    \n",
    "    features.append(dataset[\"text\"].apply(character_count))\n",
    "    features.append(dataset[\"text\"].apply(word_count))\n",
    "    features.append(dataset[\"text\"].apply(punctuation_count))\n",
    "    features.append(dataset[\"text\"].apply(digit_count))\n",
    "    features.append(dataset[\"text\"].apply(uppercase_count))\n",
    "    features.append(dataset[\"text_word_tokenized\"].apply(short_word_count))\n",
    "    features.append(dataset[\"text\"].apply(alphabet_count))\n",
    "    features.append(dataset[\"text_word_tokenized\"].apply(word_without_vowels_count))\n",
    "    features.append(dataset[\"text\"].apply(contraction_count))\n",
    "    features.append(dataset[\"text_word_tokenized\"].apply(hapax_legomenon_count))\n",
    "    features.append(dataset[\"text_word_tokenized\"].apply(mean_word_length))\n",
    "    features.append(dataset[\"text_sent_tokenized\"].apply(mean_sentence_length))\n",
    "    features.append(dataset[\"text_word_tokenized\"].apply(word_length_standard_deviation))\n",
    "    features.append(dataset[\"text_sent_tokenized\"].apply(sentence_length_standard_deviation))\n",
    "    features.append(dataset[\"text_word_tokenized\"].apply(mean_word_frequency))\n",
    "    features.append(dataset[\"text_word_tokenized\"].apply(lexical_diversity_coefficient))\n",
    "    features.append(dataset[\"text_word_tokenized\"].apply(syntactic_complexity_coefficient))\n",
    "    features.append(dataset[\"text_word_tokenized\"].apply(herdans_log_type_token_richness))\n",
    "    features.append(dataset[\"text_word_tokenized\"].apply(noun_common_ratio))\n",
    "    features.append(dataset[\"text_word_tokenized\"].apply(noun_proper_ratio))\n",
    "    features.append(dataset[\"text_word_tokenized\"].apply(adjective_base_ratio))\n",
    "    features.append(dataset[\"text_word_tokenized\"].apply(adjective_comparative_ratio))\n",
    "    features.append(dataset[\"text_word_tokenized\"].apply(adjective_superlative_ratio))\n",
    "    features.append(dataset[\"text_word_tokenized\"].apply(adverb_base_ratio))\n",
    "    features.append(dataset[\"text_word_tokenized\"].apply(adverb_comparative_ratio))\n",
    "    features.append(dataset[\"text_word_tokenized\"].apply(adverb_superlative_ratio))\n",
    "    features.append(dataset[\"text_word_tokenized\"].apply(verb_infinitive_ratio))\n",
    "    features.append(dataset[\"text_word_tokenized\"].apply(verb_present_tense_1st_2nd_person_ratio))\n",
    "    features.append(dataset[\"text_word_tokenized\"].apply(verb_present_tense_3rd_person_ratio))\n",
    "    features.append(dataset[\"text_word_tokenized\"].apply(verb_past_tense_ratio))\n",
    "    features.append(dataset[\"text_word_tokenized\"].apply(verb_present_participle))\n",
    "    features.append(dataset[\"text_word_tokenized\"].apply(verb_past_participle))\n",
    "    features.append(dataset[\"text_word_tokenized\"].apply(verb_modal_auxiliary))\n",
    "    features.append(dataset[\"text_word_tokenized\"].apply(pronoun_ratio))\n",
    "    features.append(dataset[\"text_word_tokenized\"].apply(genetive_ratio))\n",
    "    features.append(dataset[\"text_word_tokenized\"].apply(interjection_ratio))\n",
    "    features.append(dataset[\"text_word_tokenized\"].apply(foreign_word_ratio))\n",
    "    features.append(dataset[\"text_word_tokenized\"].apply(numeral_ratio))\n",
    "    features.append(dataset[\"text_word_tokenized\"].apply(parenthesis_ratio))\n",
    "    \n",
    "    feature_df = pd.DataFrame(features).T\n",
    "    \n",
    "    abt = pd.concat(dataset.index, feature_df, dataset[\"author\"])\n",
    "    \n",
    "    return abt\n",
    "\n",
    "abt = extract_abt(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abt.to_csv(os.path.join(data_dir, \"abt.csv\"))\n",
    "\n",
    "# TODO: Change POS tag functions so that it's 1 function\n",
    "#       instead of a whole bunch of functions\n",
    "#       cuz now I call the same function way too fucking times\n",
    "#       and that takes longer than the heat death of the sun"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
