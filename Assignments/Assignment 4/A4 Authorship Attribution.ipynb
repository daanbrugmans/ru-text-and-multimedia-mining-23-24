{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TxMM Assignment 4 - Authorship Attribution\n",
    "\n",
    "This "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\daanb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     C:\\Users\\daanb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\daanb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It\"s got a lot of action, a lot of heart, come...</td>\n",
       "      <td>1276465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hiro: (blushes in response) AniUniverse: (chuc...</td>\n",
       "      <td>1276465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Girl with the Baymax onesie: (stands up) Thank...</td>\n",
       "      <td>1276465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I am too! Anyways, this is where we let the fa...</td>\n",
       "      <td>1276465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Do you like sushi? Do you find your name ironi...</td>\n",
       "      <td>1276465</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text   author\n",
       "1  It\"s got a lot of action, a lot of heart, come...  1276465\n",
       "2  Hiro: (blushes in response) AniUniverse: (chuc...  1276465\n",
       "3  Girl with the Baymax onesie: (stands up) Thank...  1276465\n",
       "4  I am too! Anyways, this is where we let the fa...  1276465\n",
       "5  Do you like sushi? Do you find your name ironi...  1276465"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import string\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import nltk.tokenize.destructive\n",
    "import contractions\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"tagsets\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "\n",
    "data_dir = os.path.join(os.getcwd(), \"Data\")\n",
    "\n",
    "def load_fanfiction_data(data_purpose: str) -> pd.DataFrame:\n",
    "    path_to_data_csv = os.path.join(data_dir, f\"pan2324_{data_purpose}_data.csv\")\n",
    "    \n",
    "    return pd.read_csv(path_to_data_csv, index_col=0).sort_index()\n",
    "\n",
    "train_data = load_fanfiction_data(\"train\")\n",
    "test_data = load_fanfiction_data(\"test\")\n",
    "dev_data = load_fanfiction_data(\"dev\")\n",
    "\n",
    "train_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature extraction\n",
    "\n",
    "For this assignment, a set of different features are used. These features may be split up into a number of feature categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_sentence = \"I'm looking for a research project where I can study the language Esperanto from a linguistic perspective. That subject's called Esperantology. I think that's quite nifty :). 10/10 idea. Gabbagool!!!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Count features\n",
    "\n",
    "Count features are lexical features that the describe the count of something within the text, like the count of words or sentences in a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def character_count(text: str) -> int:\n",
    "    \"\"\"Returns the count of all characters of the given parameter string.\n",
    "    \n",
    "    Args:\n",
    "        - `text`: The string to compute the amount of characters of.\n",
    "        \n",
    "    Returns:\n",
    "        The count of all characters in the `text` parameter.\n",
    "    \"\"\"\n",
    "    \n",
    "    return len(text)\n",
    "\n",
    "def word_count(text: str) -> int:\n",
    "    \"\"\"Returns the count of all words that occur in the input string. This count is determined using NLTK's `word_tokenize` function.\n",
    "    \n",
    "    Args:\n",
    "        - `text`: The string to compute the amount of words of.\n",
    "        \n",
    "    Returns:\n",
    "        The count of all words in the `text` parameter.\n",
    "    \"\"\"\n",
    "    \n",
    "    return len(nltk.word_tokenize(text))\n",
    "\n",
    "def sentence_count(text: str) -> int:\n",
    "    \"\"\"Returns the count of all sentences that occur in the input string. This count is determined using NLTK's `sent_tokenize` function.\n",
    "    \n",
    "    Args:\n",
    "        - `text`: The string to compute the amount of sentences of.\n",
    "        \n",
    "    Returns:\n",
    "        The count of all sentences in the `text` parameter.\n",
    "    \"\"\"\n",
    "    return len(nltk.sent_tokenize(text))\n",
    "\n",
    "def punctuation_count(text: str) -> int:\n",
    "    \"\"\"Returns the count of punctuation occurrences in the input string.\n",
    "    \n",
    "    Args:\n",
    "        - `text`: The string to compute the amount of punctuation occurrences of.\n",
    "        \n",
    "    Returns:\n",
    "        The number of times a form of punctuation occurs in the `text` parameter.    \n",
    "    \"\"\"\n",
    "    \n",
    "    punctuation_count = 0\n",
    "    \n",
    "    for character in text:\n",
    "        if character in list(string.punctuation):\n",
    "            punctuation_count += 1\n",
    "    \n",
    "    return punctuation_count\n",
    "\n",
    "def digit_count(text: str) -> int:\n",
    "    \"\"\"Returns the count of individual digits occurring in the input string.\n",
    "    Note that this does not mean numbers, e.g., \"23\" will return \"2\", since the number 23 consists of two digits.\n",
    "    \n",
    "    Args:\n",
    "        - `text`: The string to compute the amount of digit occurrences of.\n",
    "        \n",
    "    Returns:\n",
    "        The number of times a digit occurs in the `text` parameter.\n",
    "    \"\"\"\n",
    "    digit_count = 0\n",
    "    \n",
    "    for character in text:\n",
    "        if character in list(string.digits):\n",
    "            digit_count += 1\n",
    "            \n",
    "    return digit_count\n",
    "\n",
    "def uppercase_count(text: str) -> int:\n",
    "    \"\"\"Returns the count of uppercase characters occurring in the input string.\n",
    "    Note that only uppercase characters that are part of ASCII are supported.\n",
    "    \n",
    "    Args:\n",
    "        - `text`: The string to compute the amount of uppercase character occurrences of.\n",
    "        \n",
    "    Returns:\n",
    "        The number of times an uppercase character occurs in the `text` parameter.\n",
    "    \"\"\"\n",
    "    uppercase_count = 0\n",
    "    \n",
    "    for character in text:\n",
    "        if character in list(string.ascii_uppercase):\n",
    "            uppercase_count += 1\n",
    "            \n",
    "    return uppercase_count\n",
    "\n",
    "def short_word_count(text: str, short_word_max_length: int = 4) -> int:\n",
    "    \"\"\"Returns the count of \"short\" words that occur in the `text` parameter.\n",
    "    The cutoff point for \"short\" words is given by the `short_word_max_length` parameter.\n",
    "    Words are extracted using NLTK's `word_tokenizer`.\n",
    "    \n",
    "    Args:\n",
    "        - `text`: The string to compute the amount of \"short\" words of.\n",
    "        - `short_word_max_length`: The maximum length of what is considered to be a \"short\" word. This length is inclusive.\n",
    "        \n",
    "    Returns:\n",
    "        The number of times a \"short\" word occurs in the `text` parameter.\n",
    "    \"\"\"\n",
    "    \n",
    "    word_tokenized_text = nltk.word_tokenize(text)\n",
    "    word_tokenized_text_no_punctuation = [word for word in word_tokenized_text if word not in list(string.punctuation)]\n",
    "    short_word_list = [word for word in word_tokenized_text_no_punctuation if len(word) <= short_word_max_length]\n",
    "    \n",
    "    return len(short_word_list)\n",
    "\n",
    "def alphabet_count(text: str, count_uppercase: bool = True, count_punctuation: bool = True, count_digits: bool = True) -> int:\n",
    "    \"\"\"Returns the length of the alphabet of the given text. A text's alphabet is defined as all unique characters that occur in that text.\n",
    "    \n",
    "    Args:\n",
    "        - `text`: The string for which to compute an alphabet for.\n",
    "        - `count_uppercase`: A boolean used to determine whether to count uppercase characters as separate from their lowercase counterparts.\n",
    "        - `count_punctuation`: A boolean used to determine whether to include punctuation in the alphabet.\n",
    "        - `count_digits`: A boolean used to determine whether to include digits in the alphabet.\n",
    "        \n",
    "    Returns:\n",
    "        The length of the alphabet of the `text` variable.\n",
    "    \"\"\"\n",
    "    if not count_uppercase:\n",
    "        text = text.lower()\n",
    "    \n",
    "    text_char_alphabet = set(text)\n",
    "    \n",
    "    if not count_punctuation:\n",
    "        text_char_alphabet = {char for char in text_char_alphabet if char not in list(string.punctuation)}\n",
    "        \n",
    "    if not count_digits:\n",
    "        text_char_alphabet = {char for char in text_char_alphabet if char not in list(string.digits)}\n",
    "    \n",
    "    return len(text_char_alphabet)\n",
    "\n",
    "def contraction_count(text: str, include_genetive_count: bool = False) -> int:\n",
    "    \"\"\"Returns the count of all contractions that occur in the given string.\n",
    "    \n",
    "    Args:\n",
    "        - `text`: The string for which to count the number of occurring contractions.\n",
    "        - `include_genetive_count`: A boolean used to determine if occurrences of the genetive should count towards the number of contractions found.\n",
    "        \n",
    "    Returns:\n",
    "        The amount of contractions that occur in the `text` variable.\n",
    "    \"\"\"\n",
    "    contraction_count = len(contractions.preview(text, 1))\n",
    "    \n",
    "    if include_genetive_count:\n",
    "        tokenized_text = nltk.tokenize.word_tokenize(text)\n",
    "        pos_tagged_text = nltk.tag.pos_tag(tokenized_text)\n",
    "        genetive_count = len([tag for _, tag in pos_tagged_text if tag == \"POS\"])\n",
    "        \n",
    "        contraction_count += genetive_count\n",
    "        \n",
    "    return contraction_count\n",
    "\n",
    "def word_without_vowels_count(text: str, include_y_as_vowel: bool = False) -> int:\n",
    "    \"\"\"Returns the count of words in the input string that do not contain vowels.\n",
    "    \n",
    "    Args:\n",
    "        - `text`: The string for which to count the number of words without vowels.\n",
    "        - `include_y_as_vowel`: A boolean used to determine if the character \"y\" should be counted as a vowel.\n",
    "        \n",
    "    Returns:\n",
    "        The number of times a word without vowels occurs in the input string.\n",
    "    \"\"\"\n",
    "    \n",
    "    word_without_vowels_count = 0\n",
    "    vowels = set(\"aeiou\")\n",
    "    \n",
    "    if include_y_as_vowel:\n",
    "        vowels.add(\"y\")\n",
    "    \n",
    "    tokenized_text = nltk.word_tokenize(text.lower())\n",
    "    tokenized_text = [word for word in tokenized_text if word not in (list(string.punctuation) or list(string.digits))]\n",
    "    \n",
    "    for word in tokenized_text:\n",
    "        vowels_in_word = vowels.intersection(word)\n",
    "        \n",
    "        if len(vowels_in_word) == 0:\n",
    "            word_without_vowels_count += 1\n",
    "    \n",
    "    return word_without_vowels_count\n",
    "\n",
    "def hapax_legomenon_count(text: str) -> int:\n",
    "    \"\"\"Returns the count of hapax legomenon in the input text.\n",
    "    A hapax legomenon is a word that occurs only once in a corpus.\n",
    "    Note that for the purposes of this function, the corpus is considered to be the input text.\n",
    "    \n",
    "    Args:\n",
    "        - `text`: The string for which to count the number of hapax legomenon.\n",
    "        \n",
    "    Returns:\n",
    "        The number of hapax legomena that were found in the input text.\n",
    "    \"\"\"\n",
    "    \n",
    "    hapax_legomenon_count = 0\n",
    "    \n",
    "    tokenized_text = nltk.word_tokenize(text.lower())\n",
    "    \n",
    "    for word in tokenized_text:\n",
    "        word_count_in_text = tokenized_text.count(word)\n",
    "        \n",
    "        if word_count_in_text == 1:\n",
    "            hapax_legomenon_count += 1\n",
    "            \n",
    "    return hapax_legomenon_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Text complexity features\n",
    "\n",
    "Text complexity features aim to describe the complexity of the given text. This may be at the lexical or the syntactical level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_word_length(text: str) -> int:\n",
    "    raise NotImplementedError()\n",
    "\n",
    "def mean_sentence_length(text: str) -> int:\n",
    "    raise NotImplementedError()\n",
    "\n",
    "def word_length_standard_deviation(text: str) -> int:\n",
    "    raise NotImplementedError()\n",
    "\n",
    "def sentence_length_standard_deviation(text: str) -> int:\n",
    "    raise NotImplementedError()\n",
    "\n",
    "def mean_word_frequency(text: str) -> int:\n",
    "    raise NotImplementedError()\n",
    "\n",
    "def lexical_diversity_coefficient(text: str) -> int:\n",
    "    raise NotImplementedError()\n",
    "\n",
    "def syntactic_complexity_coefficient(text: str) -> int:\n",
    "    raise NotImplementedError()\n",
    "\n",
    "def herdans_log_type_token_richness(text: str) -> int:\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Part-of-speech features\n",
    "\n",
    "These features describe the ratios of words that belong to a specific part-of-speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_part_of_speech_tags(text: str) -> int:\n",
    "    raise NotImplementedError()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
